= BLS Wage Data Getter
:toc:

A TypeScript application for downloading and storing wage data from the U.S. Bureau of Labor Statistics (BLS) Occupational Employment and Wage Statistics (OEWS) bulk files.

== Overview

This project loads BLS OEWS data from the bulk time series files, storing the results in a PostgreSQL database for analysis and reporting.

== Features

* Downloads OEWS bulk files and caches them locally
* Parses OEWS mapping files for occupations, industries, areas, and datatypes
* Loads OEWS series metadata and data values into PostgreSQL
* Stores data in PostgreSQL with proper schema
* Type-safe validation using TypeBox schemas
* Automated database migrations with Kysely

== Project Structure

[source,text]
----
src/
├── db/
│   ├── generated/           # Kysely-generated types
│   ├── migrations/          # Database schema migrations
│   ├── index.ts             # Database connection setup
│   └── migrate.ts           # Migration runner
├── schemas/
│   ├── bulk.ts              # TypeBox schemas for bulk files
│   ├── validate.ts          # Data validation helpers
│   └── validation.test.ts   # Schema tests
└── scripts/
    ├── apply_migrations.ts  # Run migrations via script
    ├── constants.ts         # Shared script constants
    ├── db_export.ts          # CSV export helper
    ├── test_db_connection.ts
    ├── bulk/
    │   ├── download.ts       # Download OEWS bulk files
    │   ├── ingest_mappings.ts # Load mapping tables
    │   ├── ingest_series.ts   # Load OEWS series metadata
    │   ├── ingest_data.ts     # Load OEWS data values
    │   ├── parsers.ts         # Bulk file parsing helpers
    │   └── run.ts             # Download + ingest end-to-end
    ├── meaningfulness/
    │   └── run.ts             # LLM scoring runner
└── test-utils/
    ├── testDBManager.ts       # Test database lifecycle helpers
    └── testDataSeeder/
        ├── constants.ts       # Seeder defaults
        ├── index.ts            # Seed orchestration
        └── deterministicRandom/
            ├── dRandom.ts      # Deterministic RNG
            ├── hashing.ts      # Stable hashing helpers
            └── overridableFieldHelpers.ts # Repeatable overrides
----

== Database Schema

* `oe_occupations` - OEWS occupation mapping file
* `oe_industries` - OEWS industry mapping file
* `oe_areas` - OEWS area mapping file
* `oe_areatypes` - OEWS area types
* `oe_datatypes` - OEWS data type definitions
* `oe_footnotes` - OEWS footnote definitions
* `oe_releases` - OEWS release metadata
* `oe_seasonal` - OEWS seasonality definitions
* `oe_sectors` - OEWS sector definitions
* `oe_series` - OEWS series metadata (bulk file)
* `oe_data` - OEWS data values (bulk file)
* `meaningfulness_scores` - LLM-scored occupation/industry meaningfulness

== Prerequisites

* Node.js (see `.nvmrc` for the recommended version)
** Node.js version should include TypeScript type stripping support (scripts execute .ts files directly via node; use a Node version that supports this or set NODE_OPTIONS=--experimental-strip-types)
* Docker & Docker Compose

== Setup

. Clone the repository
. Install dependencies:
+
[source,bash]
----
npm install
----
+
. Copy environment variables:
+
[source,bash]
----
cp .env.example .env
----
+


== Usage

. Start the database:
+
[source,bash]
----
npm run db:setup
----
+
This command:
+ 
* Starts PostgreSQL container
* Runs database migrations
* Generates TypeScript types
+
. Test database connection:
+
[source,bash]
----
npm run db:test:conn
----
+
. Download OEWS bulk files (this may take a while - some files are ~1.5GB):
+
[source,bash]
----
npm run bulk:download
----
+
. Ingest the downloaded data into the database:
+
[source,bash]
----
npm run bulk:ingest
----
+
. The ingest script will skip downloading if files already exist and load mappings, series metadata, and data values into the database.
+
. Ingestion order matters because of FK constraints. The `bulk:ingest` script enforces the correct order:
+
* Mapping tables (areatypes -> areas -> datatypes -> sectors -> occupations -> industries)
* Series metadata (`oe.series`) - only national-level data (state_code = "00")
* Data values (`oe.data.0.Current`) - only for series_ids in oe_series
+
NOTE: For performance, only national-level series (state_code = "00") are ingested (~2M series instead of ~6M). Data rows are filtered to match existing series_ids. This reduces data volume by ~67% while preserving all data needed for national-level analysis.
+
. Use the mapping tables to interpret codes in `oe_series` and `oe_data`:
+
[source,sql]
----
select
  s.series_id,
  o.occupation_name,
  i.industry_name,
  d.datatype_name,
  a.area_name,
  s.series_title,
  s.begin_year,
  s.end_year
from oe_series s
join oe_occupations o on o.occupation_code = s.occupation_code
join oe_industries i on i.industry_code = s.industry_code
join oe_datatypes d on d.datatype_code = s.datatype_code
join oe_areas a on a.area_code = s.area_code and a.state_code = s.state_code
limit 25;
----
+
. Score occupation/industry meaningfulness in batches (skips already-scored pairs):
+
[source,bash]
----
npm run meaningfulness:score
----
+
. Use the prefiltered view for analysis (latest year + mean annual wage only):
+
[source,sql]
----
select *
from occupation_industry_meaningfulness
where meaningfulness_score is not null
limit 25;
----
+
. Export a single table to CSV:
+
[source,bash]
----
npm run db:backup:table
----
+
. `db:backup:table` defaults to `meaningfulness_scores`, and supports `BACKUP_TABLE`, `BACKUP_PATH`, and `BACKUP_BATCH_SIZE`.
+
. Backup all tables (for restoring later):
+
[source,bash]
----
npm run db:backup
----
+
. Restore all tables from backup:
+
[source,bash]
----
npm run db:restore
----
+
. `db:backup` and `db:restore` support `BACKUP_DIR` and `BACKUP_BATCH_SIZE` environment variables.
+
. Explore the data with something like https://www.beekeeperstudio.io/[Beekeeper] or https://www.pgadmin.org/[pgAdmin]. You can use the config in `.env.example` to connect to the local database.
. Spin down the database when you're finished
+
[source,bash]
----
npm run db:down
----


== Development

* Format code:
+
[source,bash]
----
npm run format
----
+
* Run tests:
+
[source,bash]
----
npm run test
----
+
NOTE: Use `npm run test` (not `npx vitest`) so env-cmd loads `.env.test`; Vitest expects env vars to be preloaded by the script.

* Run tests with coverage:
+
[source,bash]
----
npm run test:coverage
----


== Database Management

Start database:

[source,bash]
----
npm run db:up
----

Stop database:

[source,bash]
----
npm run db:down
----

View database logs:

[source,bash]
----
npm run db:logs
----

Run migrations:

[source,bash]
----
npm run db:migrate
----

Generate TypeScript types:

[source,bash]
----
npm run db:codegen
----

== Environment Variables

* `DATABASE_URL` - PostgreSQL connection URL (defaults to local Docker container)
* `OPENAI_API_KEY` - OpenAI API token used for meaningfulness scoring
* `OPENAI_MODEL` - Model name for meaningfulness scoring (default `gpt-4.1-mini`)
* `MEANINGFULNESS_BATCH_LIMIT` - Max pairs to score per run (default `50`)
* `INGEST_BATCH_SIZE` - Batch size for series and data ingestion (default `1000` for series, `2000` for data)
* `BACKUP_TABLE` - Table name to export (default `meaningfulness_scores`)
* `BACKUP_PATH` - Output path for single table CSV export
* `BACKUP_DIR` - Directory for full backup/restore
* `BACKUP_BATCH_SIZE` - Rows per export batch (default `5000`)
* `RESTORE_BATCH_SIZE` - Rows per import batch during restore (default `1000`)
* Tests load `.env.test` via env-cmd in the npm script; running Vitest directly will skip this

== Technologies

* **Runtime**: Node.js
* **Database**: PostgreSQL with Kysely ORM
* **Validation**: TypeBox schemas
* **Testing**: Vitest
* **Formatting**: Prettier
* **Git Hooks**: Husky

== To Do

* Add https://github.com/DefinitelyTyped/DefinitelyTyped?tab=readme-ov-file#adding-tests-to-a-new-package[DefinitelyTyped] types to jstat instead of custom-written types
* Should add test that meaningfulness scores are approx normal


== License

MIT
